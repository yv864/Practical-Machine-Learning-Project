---
title: "PAW Project.Rmd"
author: "Jamin Chai"
date: "11 0ct, 2017"
output: html_document
---
This is a Prediction Assignment Writeup project for PML course on Coursera taught by JHU.

========================================================

## Background

Using devices such as Jawbone Up, Nike FuelBand and Fitbit, it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset).

## Data Loading

Load the two datasets.

```{r}
pmltraining <- read.csv('pml-training.csv')
pmltesting <- read.csv('pml-testing.csv')
```

Partition training data into two sets: one for training and the other for cross validation.

```{r}
library(caret)
set.seed(1000)
trainingIndex <- createDataPartition(pmltraining$classe, list=FALSE, p=.9)
training = pmltraining[trainingIndex,]
testing = pmltraining[-trainingIndex,]
```

Remove near-zero-variance indicators.

```{r}
library(caret)
none <- nearZeroVar(training)
training <- training[-none]
testing <- testing[-none]
pmltesting <- pmltesting[-none]
```

Filter columns to only include numeric features and outcome.

```{r}
num_features_idx = which(lapply(training,class) %in% c('numeric')  )
```

We then would like to impute missing values as many exist in our training data.

```{r}
preModel <- preProcess(training[,num_features_idx], method=c('knnImpute'))

ptraining <- cbind(training$classe, predict(preModel, training[,num_features_idx]))
ptesting <- cbind(testing$classe, predict(preModel, testing[,num_features_idx]))
prtesting <- predict(preModel, pmltesting[,num_features_idx])

#Fix Label on classe
names(ptraining)[1] <- 'classe'
names(ptesting)[1] <- 'classe'
```

## Model

Then we build a random forest model using the numerical variables provided. As we will see later, it provides considerable accuracy to predict the twenty test cases.

```{r}
library(randomForest)
rf_model  <- randomForest(classe ~ ., ptraining, ntree=500, mtry=32)
```

## Cross Validation

We are able to measure the accuracy using our training set and our cross validation set. With the training set we can detect if our model has bias due to ridgity of our mode. With the cross validation set, we are able to determine if we have variance due to overfitting.

### In-sample accuracy
```{r}
training_pred <- predict(rf_model, ptraining) 
print(confusionMatrix(training_pred, ptraining$classe))
```

The in-sample accuracy is 100% which indicates, the model does not suffer from bias.

### Out-of-sample accuracy
```{r}
testing_pred <- predict(rf_model, ptesting) 
```

Confusion Matrix: 
```{r}
print(confusionMatrix(testing_pred, ptesting$classe))
```

The cross validation accuracy is greater than 99%, which should be sufficient for predicting the twenty test observations. Based on the lower bound of the confidence interval we would expect to achieve a 98.7% classification accuracy on new data provided. 

One caveat exists that the new data must be collected and preprocessed in a manner consistent with the training data.

## Results

Applying this model to the test data provided yields 100% classification accuracy on the twenty test observations.

```{r}
results <- predict(rf_model, prtesting) 
print(results)
```
